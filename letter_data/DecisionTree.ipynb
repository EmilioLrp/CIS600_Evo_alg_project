{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEAjcnfDzUOb",
        "outputId": "488c8eb4-3b2e-4be0-f3b2-38130815f1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3e92aaa639fe>:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  training_points = np.array(letters[:15000].drop(['letter'], 1))\n",
            "<ipython-input-2-3e92aaa639fe>:16: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  test_points = np.array(letters[1500:].drop(['letter'], 1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9648648648648649\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.99      0.98      0.98       729\n",
            "           B       0.93      0.97      0.95       702\n",
            "           C       0.97      0.96      0.97       685\n",
            "           D       0.95      0.97      0.96       736\n",
            "           E       0.95      0.95      0.95       714\n",
            "           F       0.96      0.96      0.96       728\n",
            "           G       0.97      0.95      0.96       716\n",
            "           H       0.93      0.93      0.93       667\n",
            "           I       0.98      0.97      0.97       700\n",
            "           J       0.97      0.97      0.97       672\n",
            "           K       0.96      0.96      0.96       680\n",
            "           L       0.98      0.98      0.98       709\n",
            "           M       0.98      0.98      0.98       718\n",
            "           N       0.97      0.96      0.97       726\n",
            "           O       0.96      0.97      0.96       702\n",
            "           P       0.95      0.96      0.96       736\n",
            "           Q       0.96      0.96      0.96       734\n",
            "           R       0.96      0.94      0.95       707\n",
            "           S       0.96      0.95      0.96       680\n",
            "           T       0.98      0.97      0.97       738\n",
            "           U       0.97      0.98      0.98       761\n",
            "           V       0.98      0.97      0.98       708\n",
            "           W       0.99      0.98      0.99       715\n",
            "           X       0.96      0.97      0.96       728\n",
            "           Y       0.96      0.96      0.96       723\n",
            "           Z       0.97      0.97      0.97       686\n",
            "\n",
            "    accuracy                           0.96     18500\n",
            "   macro avg       0.96      0.96      0.96     18500\n",
            "weighted avg       0.96      0.96      0.96     18500\n",
            "\n",
            "[[714   0   0   1   1   0   1   2   0   0   1   1   2   0   1   0   0   0\n",
            "    0   0   1   1   0   1   2   0]\n",
            " [  0 680   0   3   0   1   0   3   1   0   0   0   0   0   1   2   0   5\n",
            "    1   2   0   1   0   1   0   1]\n",
            " [  3   0 661   0   8   1   3   1   0   0   0   1   0   1   2   0   0   0\n",
            "    0   1   1   0   0   0   2   0]\n",
            " [  0   2   0 713   0   1   0   5   1   0   0   0   0   3   2   2   0   4\n",
            "    0   0   0   0   0   1   1   1]\n",
            " [  0   1   3   0 678   4   3   0   0   0   0   2   0   0   0   1   3   3\n",
            "    5   0   0   0   0   5   0   6]\n",
            " [  0   1   1   0   1 698   0   2   3   1   0   1   0   3   0  12   1   0\n",
            "    0   1   0   0   0   1   2   0]\n",
            " [  0   3   5   0   3   1 680   2   0   3   2   1   0   0   4   2   3   1\n",
            "    1   2   1   0   1   1   0   0]\n",
            " [  0  11   0   4   3   1   1 617   0   2   6   0   0   2   2   2   0   6\n",
            "    3   0   4   1   1   1   0   0]\n",
            " [  1   1   0   0   1   1   0   0 677   3   0   0   0   1   0   1   1   0\n",
            "    2   1   0   0   0   5   1   4]\n",
            " [  1   0   1   2   3   1   0   0   3 653   0   0   0   0   1   1   0   0\n",
            "    2   0   1   0   1   0   0   2]\n",
            " [  0   3   1   1   3   2   1   7   0   0 652   0   0   0   0   1   1   3\n",
            "    1   1   0   0   0   3   0   0]\n",
            " [  0   0   3   0   1   1   1   1   0   1   1 696   0   1   1   0   0   0\n",
            "    2   0   0   0   0   0   0   0]\n",
            " [  1   0   1   0   0   0   0   0   0   2   0   0 706   2   0   0   2   1\n",
            "    0   0   1   0   2   0   0   0]\n",
            " [  0   3   1   2   0   2   0   2   0   0   2   1   2 699   4   0   0   1\n",
            "    0   0   1   2   1   0   3   0]\n",
            " [  0   1   2   3   0   0   2   3   0   0   0   1   0   0 679   0   5   0\n",
            "    0   1   2   1   1   1   0   0]\n",
            " [  0   3   0   0   0   5   1   1   2   1   0   0   0   0   3 709   2   1\n",
            "    0   0   1   1   1   2   3   0]\n",
            " [  0   2   2   4   3   0   2   1   0   0   1   0   0   0   5   0 704   1\n",
            "    2   0   5   1   0   0   0   1]\n",
            " [  0   9   1   5   0   0   0   3   0   0   5   2   1   2   2   3   3 667\n",
            "    0   0   0   1   0   2   0   1]\n",
            " [  1   2   0   2   3   1   1   5   2   2   0   1   0   0   0   0   2   0\n",
            "  649   1   0   0   0   4   0   4]\n",
            " [  1   1   0   0   0   5   0   1   0   1   0   1   0   0   1   0   0   0\n",
            "    1 717   0   0   1   0   6   2]\n",
            " [  1   0   1   2   0   0   1   1   0   0   0   0   2   2   1   0   1   1\n",
            "    0   2 744   0   0   1   1   0]\n",
            " [  0   3   0   0   0   0   1   0   0   0   0   0   0   0   0   3   2   1\n",
            "    0   0   0 690   0   0   8   0]\n",
            " [  0   0   0   0   0   1   2   0   0   0   0   0   3   2   1   0   0   1\n",
            "    0   0   0   1 703   0   1   0]\n",
            " [  0   3   0   2   3   1   0   1   0   3   7   1   0   0   0   0   0   0\n",
            "    1   0   2   0   0 704   0   0]\n",
            " [  0   1   0   3   0   1   0   2   0   1   0   0   1   0   0   2   1   0\n",
            "    3   4   1   5   0   0 697   1]\n",
            " [  0   1   0   4   4   0   0   0   0   0   0   3   0   0   0   2   1   1\n",
            "    6   0   0   0   0   1   0 663]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "letters = pd.read_csv('letter-recognition.txt')\n",
        "\n",
        "training_points = np.array(letters[:15000].drop(['letter'], 1))\n",
        "training_labels = np.array(letters[:15000]['letter'])\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(training_points,training_labels)\n",
        "\n",
        "\n",
        "test_points = np.array(letters[1500:].drop(['letter'], 1))\n",
        "test_labels = np.array(letters[1500:]['letter'])\n",
        "\n",
        "accuracy = clf.score(test_points, test_labels)\n",
        "\n",
        "print(float(accuracy))\n",
        "\n",
        "expected = test_labels\n",
        "predicted = clf.predict(test_points)\n",
        "\n",
        "# summarize the fit of the model\n",
        "print(metrics.classification_report(expected, predicted))\n",
        "print(metrics.confusion_matrix(expected, predicted))"
      ]
    }
  ]
}